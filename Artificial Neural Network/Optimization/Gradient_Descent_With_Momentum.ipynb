{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient_Descent_With_Momentum.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNusQbKkhXuzh4Ua46B8Q/7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIih8uRa2-3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import xlogy\n",
        "import h5py\n",
        "\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_RyO_lXaEPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset():\n",
        "\n",
        "  \"\"\"Loads the Cat vs Non-Cat dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_train, y_train, X_test, y_test, classes: Arrays\n",
        "      Dataset splitted into train and test with classes\n",
        "  \"\"\"\n",
        "  \n",
        "  train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
        "  train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
        "  train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n",
        "\n",
        "  test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
        "  test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
        "  test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])\n",
        "\n",
        "  classes = np.array(test_dataset[\"list_classes\"][:])\n",
        "  \n",
        "  return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dctpTbmuY8vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(Z):\n",
        "\n",
        "  \"\"\"Applies relu function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying relu function\n",
        "  \"\"\"\n",
        "  \n",
        "  return np.maximum(Z, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOVstqTPPo3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_prime(Z):\n",
        "  \n",
        "  \"\"\"Applies differentiation of relu function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying diff of relu function\n",
        "  \"\"\"\n",
        "\n",
        "  return (Z>0).astype(Z.dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgjv4Fl2nzcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(Z):\n",
        "\n",
        "  \"\"\"Applies sigmoid function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying sigmoid function\n",
        "  \"\"\"    \n",
        "  \n",
        "  return 1/(1+np.power(np.e, -Z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5hVTQHOozHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_prime(Z):\n",
        "\n",
        "  \"\"\"Applies differentiation of sigmoid function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying diff of sigmoid function\n",
        "  \"\"\"\n",
        "  \n",
        "  return Z * (1-Z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu74QjP0Oas7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def leaky_relu(Z, alpha=0.01):\n",
        "\n",
        "  \"\"\"Applies leaky relu function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "    alpha: float\n",
        "      Negative slope coefficient\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying leaky relu function\n",
        "  \"\"\"   \n",
        "\n",
        "  return np.where(Z > 0, Z, Z * alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehhZLlf_N_RA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def leaky_relu_prime(Z, alpha=0.01):\n",
        "\n",
        "  \"\"\"Applies differentiation of leaky relu function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "    alpha: float\n",
        "      Negative slope coefficient\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying diff of leaky relu function\n",
        "  \"\"\"\n",
        "\n",
        "  dz = np.ones_like(Z)\n",
        "  dz[Z < 0] = alpha\n",
        "  return dz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDAdD7oWNEWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tanh(Z):\n",
        "\n",
        "  \"\"\"Applies tanh function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying tanh function\n",
        "  \"\"\"   \n",
        "\n",
        "  return np.tanh(Z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmRt1rYRNEiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tanh_prime(Z):\n",
        "  \n",
        "  \"\"\"Applies differentiation of tanh function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying diff of tanh function\n",
        "  \"\"\"\n",
        "\n",
        "  return 1-(tanh(Z)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UXxjjGfILPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_activation_function(name):\n",
        "\n",
        "  \"\"\"Returns function corresponding to an activation name\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    name: string\n",
        "      'relu', 'leaky_relu', 'tanh' or 'sigmoid' activation\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Corresponding activation function\n",
        "  \"\"\"\n",
        "\n",
        "  if name=='relu':\n",
        "    return relu\n",
        "  elif name=='sigmoid':\n",
        "    return sigmoid\n",
        "  elif name=='leaky_relu':\n",
        "    return leaky_relu\n",
        "  elif name=='tanh':\n",
        "    return tanh\n",
        "  else:\n",
        "    raise ValueError('Only \"relu\", \"leaky_relu\", \"tanh\" and \"sigmoid\" supported')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCsMDDuEIZIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_derivative_activation_function(name):\n",
        "\n",
        "  \"\"\"Returns differentiation function corresponding to an activation name\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  name: string\n",
        "    'relu', 'leaky_relu', 'tanh' or 'sigmoid' activation\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Corresponding diff of activation function\n",
        "  \"\"\"\n",
        "\n",
        "  if name=='relu':\n",
        "    return relu_prime\n",
        "  elif name=='sigmoid':\n",
        "    return sigmoid_prime\n",
        "  elif name=='leaky_relu':\n",
        "    return leaky_relu_prime\n",
        "  elif name=='tanh':\n",
        "    return tanh_prime\n",
        "  else:\n",
        "    raise ValueError('Only \"relu\", \"leaky_relu\", \"tanh\" and \"sigmoid\" supported')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqXnbKuBPbAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_layer_weights(n_l_1, n_l, random_state=0):\n",
        "\n",
        "  \"\"\"Initializes random weights and bias for a layer l\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    n_l_1: int\n",
        "      Number of neurons in previous layer (l-1)\n",
        "    n_l_1: int\n",
        "      Number of neurons in current layer (l)\n",
        "    random_state: int\n",
        "      Random seed\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "      Contains the randomly initialized weights and bias arrays\n",
        "\n",
        "      The keys for weights and bias arrays in the dict are 'W1', 'b1', 'W2' and 'b2'\n",
        "  \"\"\"\n",
        "\n",
        "  np.random.seed(random_state)\n",
        "\n",
        "  wl = np.random.randn(n_l_1, n_l) * np.sqrt(2/n_l_1)\n",
        "  bl = np.random.randn(1, n_l) * np.sqrt(2/n_l_1)\n",
        "  Vdw = np.zeros((n_l_1, n_l))\n",
        "  Vdb = np.zeros((1, n_l))\n",
        "\n",
        "  return {'wl': wl, 'bl': bl, 'Vdw': Vdw, 'Vdb': Vdb}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSenYu7mRw0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dense:\n",
        "  \n",
        "  \"\"\"Returns a dense layer with randomly initialized weights and bias\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    input_dim: int\n",
        "      Number of neurons in previous layer.\n",
        "    units: int\n",
        "      Number of neurons in the layer.\n",
        "    activation: str\n",
        "      Activation function to use. 'relu', 'leaky_relu', 'tanh' or 'sigmoid'\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dense layer\n",
        "      An instance of the Dense layer initialized with random params.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_dim, units, activation, random_state=0):\n",
        "\n",
        "    params = initialize_layer_weights(input_dim, units, random_state)\n",
        "\n",
        "    self.units = units\n",
        "    self.W = params['wl']\n",
        "    self.b = params['bl']\n",
        "    self.activation = activation\n",
        "    self.Z = None\n",
        "    self.A = None\n",
        "    self.dz = None\n",
        "    self.da = None\n",
        "    self.dw = None\n",
        "    self.db = None\n",
        "    self.Vdw = params['Vdw']\n",
        "    self.Vdb = params['Vdb']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zYoAJIAU5In",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop(X, model):\n",
        "  \n",
        "  \"\"\"Performs forward propagation and calculates output value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    X: array_like\n",
        "      Data\n",
        "    model: list\n",
        "      List containing the layers\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Model: list\n",
        "      List containing layers with updated 'Z' and 'A'\n",
        "  \"\"\"\n",
        "\n",
        "  for i in range(len(model)):\n",
        "\n",
        "    if i==0:\n",
        "      X_l_1 = X.copy()\n",
        "    else:\n",
        "      X_l_1 = model[i-1].A\n",
        "\n",
        "    model[i].Z = np.dot(X_l_1, model[i].W) + model[i].b\n",
        "    model[i].A = get_activation_function(model[i].activation)(model[i].Z)\n",
        "    \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqX5PWcYgwjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_loss(y, model):\n",
        "\n",
        "  \"\"\"Calculate the entropy loss\n",
        "\n",
        "    Arguments\n",
        "    --------- \n",
        "    y: array-like\n",
        "      True lables\n",
        "    model: list\n",
        "      List containing the layers\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss: float\n",
        "      Entropy loss\n",
        "  \"\"\"\n",
        "\n",
        "  m = y.shape[0]\n",
        "  A = model[-1].A\n",
        "\n",
        "  return np.squeeze(-(1./m)*np.sum(np.multiply(y, np.log(A))+np.multiply(np.log(1-A), 1-y)))     # Squeeze will convert [[cost]] to 'cost' float variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmbQivksk2tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_prop(X, y, model):\n",
        "\n",
        "  \"\"\"Performs forward propagation\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    X: array_like\n",
        "      Data\n",
        "    y: array_like\n",
        "      True labels\n",
        "    model: list\n",
        "      List containing the layers\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model: list\n",
        "      List containing the layers with calculated 'dw' and 'db'\n",
        "  \"\"\"\n",
        "\n",
        "  m = X.shape[0]\n",
        "\n",
        "  for i in range(len(model)-1, -1, -1):\n",
        "\n",
        "    if i==len(model)-1:\n",
        "      model[i].dz = model[-1].A - y\n",
        "      model[i].dw = 1./m * np.dot(model[i-1].A.T, model[i].dz)\n",
        "      model[i].db = 1./m * np.sum(model[i].dz, axis=0, keepdims=True)\n",
        "\n",
        "      model[i-1].da = np.dot(model[i].dz, model[i].W.T)\n",
        "\n",
        "    else:\n",
        "\n",
        "      model[i].dz = np.multiply(np.int64(model[i].A>0), model[i].da) * get_derivative_activation_function(model[i].activation)(model[i].Z)\n",
        "\n",
        "      if i!=0:\n",
        "        model[i].dw = 1./m * np.dot(model[i-1].A.T, model[i].dz)\n",
        "      else:\n",
        "        model[i].dw = 1./m * np.dot(X.T, model[i].dz)\n",
        "      model[i].db = 1./m * np.sum(model[i].dz, axis=0, keepdims=True)\n",
        "      if i!=0:\n",
        "        model[i-1].da = np.dot(model[i].dz, model[i].W.T)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDjYMRRTsH5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_weights(model, learning_rate=0.01, beta=0.9):\n",
        "\n",
        "  \"\"\"Updates weights of the layers\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    model: list\n",
        "      List containing the layers\n",
        "    learning_rate: int, float\n",
        "      Learning rate for the weight update\n",
        "    beta: float\n",
        "      Momentum term for Gradient Descent\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model: list\n",
        "      List containing the layers\n",
        "  \"\"\"\n",
        "\n",
        "  for i in range(len(model)):\n",
        "    model[i].Vdw = beta*model[i].Vdw + (1-beta)*model[i].dw\n",
        "    model[i].Vdb = beta*model[i].Vdb + (1-beta)*model[i].db\n",
        "    model[i].W -= learning_rate*model[i].Vdw\n",
        "    model[i].b -= learning_rate*model[i].Vdb\n",
        "    \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3lz5jxHtxY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X, y, model):\n",
        "    \n",
        "  \"\"\"Using the learned parameters, predicts a class for each example in X\n",
        "  \n",
        "  Arguments\n",
        "  ---------\n",
        "  X: array_like\n",
        "    Data\n",
        "  y: array_like\n",
        "    True Labels\n",
        "  model: list\n",
        "    List containing the layers\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  predictions: array_like\n",
        "    Vector of predictions of our model\n",
        "  \"\"\"\n",
        "  \n",
        "  model1 = forward_prop(X, model.copy())\n",
        "  predictions = np.where(model1[-1].A > 0.5, 1, 0)\n",
        "  \n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyrtif1egIjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_mislabeled_images(classes, X, y, p):\n",
        "    \n",
        "  \"\"\"Plots images where predictions and truth were different.\n",
        "  \n",
        "  Arguments\n",
        "  ---------\n",
        "  classes: List\n",
        "    Output classes\n",
        "  X: array_like\n",
        "    Data\n",
        "  y: array_like\n",
        "    True Labels\n",
        "  p: array-like\n",
        "    Predictions\n",
        "  \"\"\"\n",
        "\n",
        "  a = p + y\n",
        "  mislabeled_indices = np.asarray(np.where(a == 1))\n",
        "  plt.rcParams['figure.figsize'] = (40.0, 40.0)\n",
        "  num_images = len(mislabeled_indices[0])\n",
        "  for i in range(num_images):\n",
        "      index = mislabeled_indices[0][i]\n",
        "      plt.subplot(2, num_images, i + 1)\n",
        "      plt.imshow(X[index, :].reshape(64,64,3), interpolation='nearest')\n",
        "      plt.axis('off')\n",
        "      plt.title(\"Prediction: \" + classes[int(p[index, 0])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[index, 0]].decode(\"utf-8\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfbKMqXt2V5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train, X_test, y_test, classes = load_dataset()\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqu1h_sHOB5e",
        "colab_type": "code",
        "outputId": "545c45da-fdd3-4c15-e5f6-9cc7099af4e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(209, 64, 64, 3)\n",
            "(209, 1)\n",
            "(50, 64, 64, 3)\n",
            "(50, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z666lHOVzZGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUTl1BGVE6qe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train/255.\n",
        "X_test = X_test/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azYPeMT8apak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_state = 42\n",
        "epochs = 1500\n",
        "learning_rate = 5e-3\n",
        "momentum = 0.9\n",
        "\n",
        "model = []\n",
        "model.append(Dense(input_dim = X_train.shape[1], units=20, activation='relu', random_state=random_state))\n",
        "model.append(Dense(input_dim = 20, units=16, activation='relu', random_state=random_state))\n",
        "model.append(Dense(input_dim = 16, units=8, activation='relu', random_state=random_state))\n",
        "model.append(Dense(input_dim = 8, units=4, activation='relu', random_state=random_state))\n",
        "model.append(Dense(input_dim = 4, units=y_train.shape[1], activation='sigmoid', random_state=random_state))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1nRMqVqtQln",
        "colab_type": "code",
        "outputId": "c17e9a30-267f-455d-9358-c644dae88620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "for i in range(epochs):\n",
        "  model = forward_prop(X_train, model)\n",
        "  loss = calculate_loss(y_train, model)\n",
        "  model = backward_prop(X_train, y_train, model)\n",
        "  model = update_weights(model, learning_rate, momentum)\n",
        "\n",
        "  if i%(epochs/10)==0:\n",
        "    print('Epoch: {}\\tLoss: {:.6f}\\tTrain Accuracy: {:.3f}\\tTest Accuracy: {:.3f}'.format(i, loss, accuracy_score(y_train, predict(X_train, y_train, model)), accuracy_score(y_test, predict(X_test, y_test, model))))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tLoss: 0.738440\tTrain Accuracy: 0.498\tTest Accuracy: 0.340\n",
            "Epoch: 150\tLoss: 0.659858\tTrain Accuracy: 0.660\tTest Accuracy: 0.340\n",
            "Epoch: 300\tLoss: 0.647145\tTrain Accuracy: 0.689\tTest Accuracy: 0.340\n",
            "Epoch: 450\tLoss: 0.589539\tTrain Accuracy: 0.770\tTest Accuracy: 0.440\n",
            "Epoch: 600\tLoss: 0.549697\tTrain Accuracy: 0.799\tTest Accuracy: 0.520\n",
            "Epoch: 750\tLoss: 0.509250\tTrain Accuracy: 0.813\tTest Accuracy: 0.480\n",
            "Epoch: 900\tLoss: 0.462961\tTrain Accuracy: 0.861\tTest Accuracy: 0.620\n",
            "Epoch: 1050\tLoss: 0.404637\tTrain Accuracy: 0.904\tTest Accuracy: 0.800\n",
            "Epoch: 1200\tLoss: 0.350350\tTrain Accuracy: 0.938\tTest Accuracy: 0.820\n",
            "Epoch: 1350\tLoss: 0.307516\tTrain Accuracy: 0.947\tTest Accuracy: 0.820\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}