{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient_Checking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIih8uRa2-3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import xlogy\n",
        "import h5py\n",
        "\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QEsTKFAWh3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "\n",
        "  \"\"\"Loads the dataset and initialize the model\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X: array\n",
        "      Input features\n",
        "    y: array\n",
        "      Target variables\n",
        "    model: list\n",
        "      List containing the layers\n",
        "  \"\"\"  \n",
        "\n",
        "  # #XOR\n",
        "  X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "  y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "  # #AND\n",
        "  # X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "  # y = np.array([[0],[0],[0],[1]])\n",
        "\n",
        "  model = []\n",
        "  model.append(Dense(X.shape[1], 3, 'relu', 1))\n",
        "  model.append(Dense(3, y.shape[1], 'sigmoid', 1))\n",
        "\n",
        "  return X, y, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dctpTbmuY8vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(Z):\n",
        "\n",
        "  \"\"\"Applies relu function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying relu function\n",
        "  \"\"\"\n",
        "  \n",
        "  return np.maximum(Z, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOVstqTPPo3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_prime(Z):\n",
        "  \n",
        "  \"\"\"Applies differentiation of relu function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying diff of relu function\n",
        "  \"\"\"\n",
        "\n",
        "  return (Z>0).astype(Z.dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgjv4Fl2nzcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(Z):\n",
        "\n",
        "  \"\"\"Applies sigmoid function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying sigmoid function\n",
        "  \"\"\"    \n",
        "  \n",
        "  return 1/(1+np.power(np.e, -Z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5hVTQHOozHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_prime(Z):\n",
        "\n",
        "  \"\"\"Applies differentiation of sigmoid function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying diff of sigmoid function\n",
        "  \"\"\"\n",
        "  \n",
        "  return (1-np.power(Z, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu74QjP0Oas7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def leaky_relu(Z, alpha=0.01):\n",
        "\n",
        "  \"\"\"Applies leaky relu function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "    alpha: float\n",
        "      Negative slope coefficient\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying leaky relu function\n",
        "  \"\"\"   \n",
        "\n",
        "  return np.where(Z > 0, Z, Z * alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehhZLlf_N_RA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def leaky_relu_prime(Z, alpha=0.01):\n",
        "\n",
        "  \"\"\"Applies differentiation of leaky relu function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "    alpha: float\n",
        "      Negative slope coefficient\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying diff of leaky relu function\n",
        "  \"\"\"\n",
        "\n",
        "  dz = np.ones_like(Z)\n",
        "  dz[Z < 0] = alpha\n",
        "  return dz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDAdD7oWNEWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tanh(Z):\n",
        "\n",
        "  \"\"\"Applies tanh function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying tanh function\n",
        "  \"\"\"   \n",
        "\n",
        "  return np.tanh(Z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmRt1rYRNEiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tanh_prime(Z):\n",
        "  \n",
        "  \"\"\"Applies differentiation of tanh function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying diff of tanh function\n",
        "  \"\"\"\n",
        "\n",
        "  return 1-(tanh(Z)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UXxjjGfILPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_activation_function(name):\n",
        "\n",
        "  \"\"\"Returns function corresponding to an activation name\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    name: string\n",
        "      'relu', 'leaky_relu', 'tanh' or 'sigmoid' activation\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Corresponding activation function\n",
        "  \"\"\"\n",
        "\n",
        "  if name=='relu':\n",
        "    return relu\n",
        "  elif name=='sigmoid':\n",
        "    return sigmoid\n",
        "  elif name=='leaky_relu':\n",
        "    return leaky_relu\n",
        "  elif name=='tanh':\n",
        "    return tanh\n",
        "  else:\n",
        "    raise ValueError('Only \"relu\", \"leaky_relu\", \"tanh\" and \"sigmoid\" supported')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCsMDDuEIZIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_derivative_activation_function(name):\n",
        "\n",
        "  \"\"\"Returns differentiation function corresponding to an activation name\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  name: string\n",
        "    'relu', 'leaky_relu', 'tanh' or 'sigmoid' activation\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Corresponding diff of activation function\n",
        "  \"\"\"\n",
        "\n",
        "  if name=='relu':\n",
        "    return relu_prime\n",
        "  elif name=='sigmoid':\n",
        "    return sigmoid_prime\n",
        "  elif name=='leaky_relu':\n",
        "    return leaky_relu_prime\n",
        "  elif name=='tanh':\n",
        "    return tanh_prime\n",
        "  else:\n",
        "    raise ValueError('Only \"relu\", \"leaky_relu\", \"tanh\" and \"sigmoid\" supported')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjdPMct-ZRJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vector_from_model(model, params_or_grads):\n",
        "\n",
        "  \"\"\"Make a single vector of values from a model list\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    model: list\n",
        "      List containing the layers\n",
        "    params_or_grads: str\n",
        "      Parameters to be converted to dictionary.\n",
        "      Only 'params' or 'grads' is allowed.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    vector: array\n",
        "      Vector containing the values\n",
        "    keys: list\n",
        "      Keys corresponding to the values in the vector\n",
        "    shapes: list\n",
        "      Shapes of the parameters\n",
        "    activations: list\n",
        "      Activation functions of the parameters' corresponding layers\n",
        "  \"\"\"\n",
        "\n",
        "  if params_or_grads=='params':\n",
        "    info = ['W', 'b']\n",
        "  elif params_or_grads=='grads':\n",
        "    info = ['dw', 'db']\n",
        "  else:\n",
        "    raise ValueError('Only \"params\" and \"grads\" allowed')\n",
        "\n",
        "  keys = []\n",
        "  shapes = []\n",
        "  activations = []\n",
        "\n",
        "  for i, layer in enumerate(model):\n",
        "    param_1 = getattr(layer, info[0])    # W or dw\n",
        "    param_2 = getattr(layer, info[1])    # b or db\n",
        "    shapes.append(param_1.shape)\n",
        "    shapes.append(param_2.shape)\n",
        "    vector_1 = np.array(param_1).reshape(-1, 1)\n",
        "    vector_2 = np.array(param_2).reshape(-1, 1)\n",
        "\n",
        "    if i==0:\n",
        "      vector = vector_1\n",
        "    else:\n",
        "      vector = np.concatenate((vector, vector_1))\n",
        "    \n",
        "    vector = np.concatenate((vector, vector_2))\n",
        "    keys.append(info[0]+str(i+1))\n",
        "    keys.append(info[1]+str(i+1))\n",
        "    activations.append(layer.activation)\n",
        "\n",
        "  return vector, keys, shapes, activations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfFxsGL4hOLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_from_vector(vector, keys, shapes, activations):\n",
        "\n",
        "  \"\"\"Make a single vector of values from a model list\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    vector: array\n",
        "      Vector containing the values\n",
        "    keys: list\n",
        "      Keys corresponding to the values in the vector\n",
        "    shapes: list\n",
        "      Shapes of the parameters\n",
        "    activations: list\n",
        "      Activation functions of the parameters' corresponding layers\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model: list\n",
        "      List containing the layers\n",
        "  \"\"\"\n",
        "\n",
        "  model = []\n",
        "  v = vector.copy()\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  for i in range(0, len(keys), 2):\n",
        "    d = Dense(*shapes[i], activations[count])\n",
        "    k = ''.join(filter(str.isalpha, keys[i]))\n",
        "    setattr(d, k, v[:np.multiply(*shapes[i])].reshape(shapes[i]))\n",
        "    v = v[np.multiply(*shapes[i]):]\n",
        "    k = ''.join(filter(str.isalpha, keys[i+1]))\n",
        "    setattr(d, k, v[:np.multiply(*shapes[i+1])].reshape(shapes[i+1]))\n",
        "    v = v[np.multiply(*shapes[i+1]):]\n",
        "    model.append(d)\n",
        "    count += 1\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqXnbKuBPbAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_layer_weights(n_l_1, n_l, random_state=0):\n",
        "\n",
        "  \"\"\"Initializes random weights and bias for a layer l\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    n_l_1: int\n",
        "      Number of neurons in previous layer (l-1)\n",
        "    n_l_1: int\n",
        "      Number of neurons in current layer (l)\n",
        "    random_state: int\n",
        "      Random seed\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "      Contains the randomly initialized weights and bias arrays\n",
        "\n",
        "      The keys for weights and bias arrays in the dict are 'W1', 'b1', 'W2' and 'b2'\n",
        "  \"\"\"\n",
        "\n",
        "  np.random.seed(random_state)\n",
        "\n",
        "  wl = np.random.randn(n_l_1, n_l) / np.sqrt(n_l_1)\n",
        "  bl = np.random.randn(1, n_l) / np.sqrt(n_l_1)\n",
        "\n",
        "  return {'wl': wl, 'bl': bl}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSenYu7mRw0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dense:\n",
        "  \n",
        "  \"\"\"Returns a dense layer with randomly initialized weights and bias\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    input_dim: int\n",
        "      Number of neurons in previous layer.\n",
        "    units: int\n",
        "      Number of neurons in the layer.\n",
        "    activation: str\n",
        "      Activation function to use. 'relu', 'leaky_relu', 'tanh' or 'sigmoid'\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dense layer\n",
        "      An instance of the Dense layer initialized with random params.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_dim, units, activation, random_state=0):\n",
        "\n",
        "    params = initialize_layer_weights(input_dim, units, random_state)\n",
        "\n",
        "    self.units = units\n",
        "    self.W = params['wl']\n",
        "    self.b = params['bl']\n",
        "    self.activation = activation\n",
        "    self.Z = None\n",
        "    self.A = None\n",
        "    self.dz = None\n",
        "    self.da = None\n",
        "    self.dw = None\n",
        "    self.db = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zYoAJIAU5In",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop(X, model):\n",
        "  \n",
        "  \"\"\"Performs forward propagation and calculates output value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    X: array_like\n",
        "      Data\n",
        "    model: list\n",
        "      List containing the layers\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Model: list\n",
        "      List containing layers with updated 'Z' and 'A'\n",
        "  \"\"\"\n",
        "\n",
        "  for i in range(len(model)):\n",
        "\n",
        "    if i==0:\n",
        "      X_l_1 = X.copy()\n",
        "    else:\n",
        "      X_l_1 = model[i-1].A\n",
        "\n",
        "    model[i].Z = np.dot(X_l_1, model[i].W) + model[i].b\n",
        "    model[i].A = get_activation_function(model[i].activation)(model[i].Z)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqX5PWcYgwjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_loss(y, model):\n",
        "\n",
        "  \"\"\"Calculate the entropy loss\n",
        "\n",
        "    Arguments\n",
        "    --------- \n",
        "    y: array-like\n",
        "      True lables\n",
        "    model: list\n",
        "      List containing the layers\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss: float\n",
        "      Entropy loss\n",
        "  \"\"\"\n",
        "\n",
        "  m = y.shape[0]\n",
        "  A = model[-1].A\n",
        "\n",
        "  return np.squeeze(-(1./m)*np.sum(np.multiply(np.log(A), y)+np.multiply(np.log(1-A), 1-y)))     # Squeeze will convert [[cost]] to 'cost' float variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmbQivksk2tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_prop(X, y, model):\n",
        "\n",
        "  \"\"\"Performs backward propagation\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    X: array_like\n",
        "      Data\n",
        "    y: array_like\n",
        "      True labels\n",
        "    model: list\n",
        "      List containing the layers\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model: list\n",
        "      List containing the layers with calculated 'dw' and 'db'\n",
        "  \"\"\"\n",
        "\n",
        "  m = X.shape[0]\n",
        "\n",
        "  for i in range(len(model)-1, -1, -1):\n",
        "\n",
        "    if i==len(model)-1:\n",
        "      model[i].dz = model[-1].A - y\n",
        "      model[i].dw = 1./m * np.dot(model[i-1].A.T, model[i].dz)\n",
        "      model[i].db = 1./m * np.sum(model[i].dz, axis=0, keepdims=True)\n",
        "\n",
        "      model[i-1].da = np.dot(model[i].dz, model[i].W.T)\n",
        "\n",
        "    else:\n",
        "\n",
        "      model[i].dz = np.multiply(np.int64(model[i].A>0), model[i].da) * get_derivative_activation_function(model[i].activation)(model[i].Z)\n",
        "      \n",
        "      if i!=0:\n",
        "        model[i].dw = 1./m * np.dot(model[i-1].A.T, model[i].dz)\n",
        "      else:\n",
        "        model[i].dw = 1./m * np.dot(X.T, model[i].dz)\n",
        "      model[i].db = 1./m * np.sum(model[i].dz, axis=0, keepdims=True)\n",
        "      if i!=0:\n",
        "        model[i-1].da = np.dot(model[i].dz, model[i].W.T)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDjYMRRTsH5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_weights(model, learning_rate=0.01):\n",
        "\n",
        "  \"\"\"Updates weights of the layers\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    model: list\n",
        "      List containing the layers\n",
        "    learning_rate: int, float\n",
        "      Learning rate for the weight update\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model: list\n",
        "      List containing the layers\n",
        "  \"\"\"\n",
        "\n",
        "  for i in range(len(model)):\n",
        "    model[i].W -= learning_rate*model[i].dw\n",
        "    model[i].b -= learning_rate*model[i].db\n",
        "    \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au6ZnBg2YZdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_checking(model, X, y, epsilon=1e-7):\n",
        "\n",
        "  \"\"\"Checks whether the implementation of the backpropagation is correct or not\n",
        "  \n",
        "  Arguments\n",
        "  ---------\n",
        "  model: list\n",
        "    List containing the layers\n",
        "  X: array_like\n",
        "    Data\n",
        "  y: array_like\n",
        "    True Labels\n",
        "  epsilon: float\n",
        "    Tiny shift to the input to compute approximated gradient\n",
        "    \n",
        "  \"\"\"\n",
        "\n",
        "  param_vector, param_keys, param_shapes, param_activations = vector_from_model(model, 'params')\n",
        "  grad_vector, grad_keys, grad_shapes, grad_activations = vector_from_model(model, 'grads')\n",
        "  param_num = param_vector.shape[0]\n",
        "  J_plus = np.zeros((param_num, 1))\n",
        "  J_minus = np.zeros((param_num, 1))\n",
        "  grad_approx = np.zeros((param_num, 1))\n",
        "\n",
        "  for i in range(param_num):\n",
        "    theta_plus = param_vector.copy()\n",
        "    theta_plus[i][0] += epsilon\n",
        "    J_plus[i] = calculate_loss(y, forward_prop(X, model_from_vector(theta_plus, param_keys, param_shapes, param_activations)))\n",
        "    \n",
        "    theta_minus = param_vector.copy()\n",
        "    theta_minus[i][0] -= epsilon\n",
        "    J_minus[i] = calculate_loss(y, forward_prop(X, model_from_vector(theta_minus, param_keys, param_shapes, param_activations)))\n",
        "    \n",
        "    grad_approx[i] = (J_plus[i]-J_minus[i]) / (2*epsilon)\n",
        "\n",
        "  diff = np.linalg.norm(grad_vector - grad_approx) / (np.linalg.norm(grad_vector) + np.linalg.norm(grad_approx))   \n",
        "\n",
        "  if diff>2e-7:\n",
        "    print('Wrong implementation of Gradient Descent:')\n",
        "  else:\n",
        "    print('Correct implementation of Gradient Descent')\n",
        "  print('Difference between gradient and gradient approximation:', diff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEUZtHbKWaSv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "27f30c75-0aff-4927-ca44-93a8fee69478"
      },
      "source": [
        "X, y, model = load_data()\n",
        "\n",
        "model = forward_prop(X, model)\n",
        "model = backward_prop(X, y, model)\n",
        "gradient_checking(model, X, y, epsilon=1e-7)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct implementation of Gradient Descent\n",
            "Difference between gradient and gradient approximation: 2.0025630545815287e-09\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}