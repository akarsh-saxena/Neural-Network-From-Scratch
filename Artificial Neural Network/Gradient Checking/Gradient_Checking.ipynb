{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient_Checking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIih8uRa2-3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import xlogy\n",
        "import h5py\n",
        "\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_RyO_lXaEPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset():\n",
        "\n",
        "  \"\"\"Loads the Cat vs Non-Cat dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_train, y_train, X_test, y_test, classes: Arrays\n",
        "      Dataset splitted into train and test with classes\n",
        "  \"\"\"\n",
        "  \n",
        "  train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
        "  train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
        "  train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n",
        "\n",
        "  test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
        "  test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
        "  test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])\n",
        "\n",
        "  classes = np.array(test_dataset[\"list_classes\"][:])\n",
        "  \n",
        "  return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dctpTbmuY8vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(Z):\n",
        "\n",
        "  \"\"\"Applies relu function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying relu function\n",
        "  \"\"\"\n",
        "  \n",
        "  return np.maximum(Z, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOVstqTPPo3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_prime(Z):\n",
        "  \n",
        "  \"\"\"Applies differentiation of relu function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying diff of relu function\n",
        "  \"\"\"\n",
        "\n",
        "  return (Z>0).astype(Z.dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgjv4Fl2nzcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(Z):\n",
        "\n",
        "  \"\"\"Applies sigmoid function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying sigmoid function\n",
        "  \"\"\"    \n",
        "  \n",
        "  return 1/(1+np.power(np.e, -Z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5hVTQHOozHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_prime(Z):\n",
        "\n",
        "  \"\"\"Applies differentiation of sigmoid function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying diff of sigmoid function\n",
        "  \"\"\"\n",
        "  \n",
        "  return (1-np.power(Z, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu74QjP0Oas7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def leaky_relu(Z, alpha=0.01):\n",
        "\n",
        "  \"\"\"Applies leaky relu function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "    alpha: float\n",
        "      Negative slope coefficient\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying leaky relu function\n",
        "  \"\"\"   \n",
        "\n",
        "  return np.where(Z > 0, Z, Z * alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehhZLlf_N_RA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def leaky_relu_prime(Z, alpha=0.01):\n",
        "\n",
        "  \"\"\"Applies differentiation of leaky relu function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "    alpha: float\n",
        "      Negative slope coefficient\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying diff of leaky relu function\n",
        "  \"\"\"\n",
        "\n",
        "  dz = np.ones_like(Z)\n",
        "  dz[Z < 0] = alpha\n",
        "  return dz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDAdD7oWNEWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tanh(Z):\n",
        "\n",
        "  \"\"\"Applies tanh function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying tanh function\n",
        "  \"\"\"   \n",
        "\n",
        "  return np.tanh(Z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmRt1rYRNEiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tanh_prime(Z):\n",
        "  \n",
        "  \"\"\"Applies differentiation of tanh function to an array/value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    Z: float/int/array_like\n",
        "      Original Value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A: same shape as input\n",
        "      Value after applying diff of tanh function\n",
        "  \"\"\"\n",
        "\n",
        "  return 1-(tanh(Z)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UXxjjGfILPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_activation_function(name):\n",
        "\n",
        "  \"\"\"Returns function corresponding to an activation name\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    name: string\n",
        "      'relu', 'leaky_relu', 'tanh' or 'sigmoid' activation\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Corresponding activation function\n",
        "  \"\"\"\n",
        "\n",
        "  if name=='relu':\n",
        "    return relu\n",
        "  elif name=='sigmoid':\n",
        "    return sigmoid\n",
        "  elif name=='leaky_relu':\n",
        "    return leaky_relu\n",
        "  elif name=='tanh':\n",
        "    return tanh\n",
        "  else:\n",
        "    raise ValueError('Only \"relu\", \"leaky_relu\", \"tanh\" and \"sigmoid\" supported')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCsMDDuEIZIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_derivative_activation_function(name):\n",
        "\n",
        "  \"\"\"Returns differentiation function corresponding to an activation name\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  name: string\n",
        "    'relu', 'leaky_relu', 'tanh' or 'sigmoid' activation\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Corresponding diff of activation function\n",
        "  \"\"\"\n",
        "\n",
        "  if name=='relu':\n",
        "    return relu_prime\n",
        "  elif name=='sigmoid':\n",
        "    return sigmoid_prime\n",
        "  elif name=='leaky_relu':\n",
        "    return leaky_relu_prime\n",
        "  elif name=='tanh':\n",
        "    return tanh_prime\n",
        "  else:\n",
        "    raise ValueError('Only \"relu\", \"leaky_relu\", \"tanh\" and \"sigmoid\" supported')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqXnbKuBPbAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_layer_weights(n_l_1, n_l, random_state=0):\n",
        "\n",
        "  \"\"\"Initializes random weights and bias for a layer l\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    n_l_1: int\n",
        "      Number of neurons in previous layer (l-1)\n",
        "    n_l_1: int\n",
        "      Number of neurons in current layer (l)\n",
        "    random_state: int\n",
        "      Random seed\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "      Contains the randomly initialized weights and bias arrays\n",
        "\n",
        "      The keys for weights and bias arrays in the dict are 'W1', 'b1', 'W2' and 'b2'\n",
        "  \"\"\"\n",
        "\n",
        "  np.random.seed(random_state)\n",
        "\n",
        "  wl = np.random.randn(n_l_1, n_l) / np.sqrt(n_l_1)\n",
        "  bl = np.random.randn(1, n_l) / np.sqrt(n_l_1)\n",
        "\n",
        "  return {'wl': wl, 'bl': bl}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSenYu7mRw0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dense:\n",
        "  \n",
        "  \"\"\"Returns a dense layer with randomly initialized weights and bias\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    input_dim: int\n",
        "      Number of neurons in previous layer.\n",
        "    units: int\n",
        "      Number of neurons in the layer.\n",
        "    activation: str\n",
        "      Activation function to use. 'relu', 'leaky_relu', 'tanh' or 'sigmoid'\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dense layer\n",
        "      An instance of the Dense layer initialized with random params.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_dim, units, activation, random_state=0):\n",
        "\n",
        "    params = initialize_layer_weights(input_dim, units, random_state)\n",
        "\n",
        "    self.units = units\n",
        "    self.W = params['wl']\n",
        "    self.b = params['bl']\n",
        "    self.activation = activation\n",
        "    self.Z = None\n",
        "    self.A = None\n",
        "    self.dz = None\n",
        "    self.da = None\n",
        "    self.dw = None\n",
        "    self.db = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zYoAJIAU5In",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop(X, model):\n",
        "  \n",
        "  \"\"\"Performs forward propagation and calculates output value\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    X: array_like\n",
        "      Data\n",
        "    model: list\n",
        "      List containing the layers\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Model: list\n",
        "      List containing layers with updated 'Z' and 'A'\n",
        "  \"\"\"\n",
        "\n",
        "  for i in range(len(model)):\n",
        "\n",
        "    if i==0:\n",
        "      X_l_1 = X.copy()\n",
        "    else:\n",
        "      X_l_1 = model[i-1].A\n",
        "\n",
        "    model[i].Z = np.dot(X_l_1, model[i].W) + model[i].b\n",
        "    model[i].A = get_activation_function(model[i].activation)(model[i].Z)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqX5PWcYgwjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_loss(y, model):\n",
        "\n",
        "  \"\"\"Calculate the entropy loss\n",
        "\n",
        "    Arguments\n",
        "    --------- \n",
        "    y: array-like\n",
        "      True lables\n",
        "    model: list\n",
        "      List containing the layers\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss: float\n",
        "      Entropy loss\n",
        "  \"\"\"\n",
        "\n",
        "  m = y.shape[0]\n",
        "  A = model[-1].A\n",
        "\n",
        "  return np.squeeze(-(1./m)*np.sum(np.multiply(np.log(A), y)+np.multiply(np.log(1-A), 1-y)))     # Squeeze will convert [[cost]] to 'cost' float variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmbQivksk2tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_prop(X, y, model):\n",
        "\n",
        "  \"\"\"Performs backward propagation\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    X: array_like\n",
        "      Data\n",
        "    y: array_like\n",
        "      True labels\n",
        "    model: list\n",
        "      List containing the layers\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model: list\n",
        "      List containing the layers with calculated 'dw' and 'db'\n",
        "  \"\"\"\n",
        "\n",
        "  m = X.shape[0]\n",
        "\n",
        "  for i in range(len(model)-1, -1, -1):\n",
        "\n",
        "    if i==len(model)-1:\n",
        "      model[i].dz = model[-1].A - y\n",
        "      model[i].dw = 1./m * np.dot(model[i-1].A.T, model[i].dz)\n",
        "      model[i].db = 1./m * np.sum(model[i].dz, axis=0, keepdims=True)\n",
        "\n",
        "      model[i-1].da = np.dot(model[i].dz, model[i].W.T)\n",
        "\n",
        "    else:\n",
        "\n",
        "      model[i].dz = np.multiply(np.int64(model[i].A>0), model[i].da) * get_derivative_activation_function(model[i].activation)(model[i].Z)\n",
        "      \n",
        "      if i!=0:\n",
        "        model[i].dw = 1./m * np.dot(model[i-1].A.T, model[i].dz)\n",
        "      else:\n",
        "        model[i].dw = 1./m * np.dot(X.T, model[i].dz)\n",
        "      model[i].db = 1./m * np.sum(model[i].dz, axis=0, keepdims=True)\n",
        "      if i!=0:\n",
        "        model[i-1].da = np.dot(model[i].dz, model[i].W.T)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDjYMRRTsH5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_weights(model, learning_rate=0.01):\n",
        "\n",
        "  \"\"\"Updates weights of the layers\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    model: list\n",
        "      List containing the layers\n",
        "    learning_rate: int, float\n",
        "      Learning rate for the weight update\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model: list\n",
        "      List containing the layers\n",
        "  \"\"\"\n",
        "\n",
        "  for i in range(len(model)):\n",
        "    model[i].W -= learning_rate*model[i].dw\n",
        "    model[i].b -= learning_rate*model[i].db\n",
        "    \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3lz5jxHtxY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X, y, model):\n",
        "    \n",
        "    \"\"\"Using the learned parameters, predicts a class for each example in X\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    X: array_like\n",
        "      Data\n",
        "    y: array_like\n",
        "      True Labels\n",
        "    model: list\n",
        "      List containing the layers\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    predictions: array_like\n",
        "      Vector of predictions of our model\n",
        "    \"\"\"\n",
        "    \n",
        "    model1 = forward_prop(X, model.copy())\n",
        "    predictions = np.where(model1[-1].A > 0.5, 1, 0)\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyrtif1egIjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_decision_boundary(model, X, y):\n",
        "\n",
        "    \"\"\"\n",
        "    Plots decision boundary for a dataset and model\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    model: function\n",
        "      Function for getting predictions\n",
        "    X: array_like\n",
        "      Data\n",
        "    y: array_like\n",
        "      True Labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Set min and max values and give it some padding\n",
        "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
        "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
        "    h = 0.02\n",
        "\n",
        "    # Generate a grid of points with distance h between them\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Predict the function value for the whole grid\n",
        "    Z = predict(np.c_[xx.ravel(), yy.ravel()], y, model)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot the contour and training examples\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
        "    plt.ylabel('x2')\n",
        "    plt.xlabel('x1')\n",
        "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}