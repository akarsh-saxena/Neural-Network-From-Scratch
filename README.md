# Neural Network From Scratch

This repository contains my implementation of Neural Networks which I follow from Andrew NG's Deep Learning Specialization course on Coursera.

<br />

## Content

- [**Logistic Regression as a Neural Network**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/tree/master/Artificial%20Neural%20Network/Simple%20Neural%20Network)
  <br />
  Neural Network having a single layer with only one neuron.
  <br /><br />
- [**Shallow Neural Network**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/tree/master/Artificial%20Neural%20Network/Shallow%20Neural%20Network)
  <br />
  Neural Network having a single layer with 'n' neurons.
  <br /><br />
- [**Deep Neural Network**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/tree/master/Artificial%20Neural%20Network/Deep%20Neural%20Network)
  <br />
  Neural Network having 'L' layers with multiple neurons each.
    <br /><br />
- [**L2 Regularization**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/blob/master/Artificial%20Neural%20Network/Regularization/L2_Regularization.ipynb)
  <br />
  Implementation of L2 Regularization (Ridge Regression).
  <br /><br />
- [**Dropout**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/blob/master/Artificial%20Neural%20Network/Regularization/Dropout.ipynb)
  <br />
  Implementation of Dropout.
  <br /><br />
- [**Gradient Checking**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/tree/master/Artificial%20Neural%20Network/Gradient%20Checking)
  <br />
  Gradient checking to check whether the implementation of gradient descent algorithm is correct or not.
  <br /><br />
- [**Exponentially Weighted MA (EWMA)**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/blob/master/Artificial%20Neural%20Network/Optimization/Exponentially_Weighted_MA.ipynb)
  <br />
  Implementation of Exponentially Weighted Moving Average.
  <br /><br />
- [**EWMA in Gradient Descent**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/blob/master/Artificial%20Neural%20Network/Optimization/EWMA_in_GD.ipynb)
  <br />
  Implementation of Exponentially Weighted Moving Average in Gradient Descent.
  <br /><br />
- [**Mini Batch Gradient Descent**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/blob/master/Artificial%20Neural%20Network/Optimization/Mini_Batch_Gradient_Descent.ipynb)
  <br />
  Implementation of gradient descent with mini batches
  <br /><br />
- [**Gradient Descent With Momentum**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/blob/master/Artificial%20Neural%20Network/Optimization/Gradient_Descent_With_Momentum.ipynb)
  <br />
  Implementation of Gradient Descent with Momentum.
