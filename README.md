# Neural Network From Scratch

This repository contains my implementation of Neural Networks which I follow from Andrew NG's Deep Learning Specialization course on Coursera.

<br />

## Content

- [**Logistic Regression as a Neural Network**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/tree/master/Artificial%20Neural%20Network/Simple%20Neural%20Network)
  <br />
  Neural Network having a single layer with only one neuron.
  <br /><br />
- [**Shallow Neural Network**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/tree/master/Artificial%20Neural%20Network/Shallow%20Neural%20Network)
  <br />
  Neural Network having a single layer with 'n' neurons.
  <br /><br />
- [**Deep Neural Network**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/tree/master/Artificial%20Neural%20Network/Deep%20Neural%20Network)
  <br />
  Neural Network having 'L' layers with multiple neurons each.
    <br /><br />
- [**L2 Regularization**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/blob/master/Artificial%20Neural%20Network/Regularization/L2_Regularization.ipynb)
  <br />
  Implementation of L2 Regularization (Ridge Regression).
  <br /><br />
- [**Dropout**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/blob/master/Artificial%20Neural%20Network/Regularization/Dropout.ipynb)
  <br />
  Implementation of Dropout.
  <br /><br />
- [**Gradient Checking**](https://github.com/akarsh-saxena/Neural-Network-From-Scratch/tree/master/Artificial%20Neural%20Network/Gradient%20Checking)
  <br />
  Gradient checking to check whether the implementation of gradient descent algorithm is correct or not.
